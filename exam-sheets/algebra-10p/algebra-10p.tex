\documentclass[landscape, 8pt]{extarticle}
\usepackage{geometry}
% \usepackage{showframe}
\usepackage[dvipsnames]{xcolor}

\colorlet{colour1}{Red}
\colorlet{colour2}{Green}
\colorlet{colour3}{Cerulean}

\geometry{
    a4paper, 
    margin=0.17in
}

\pretolerance=0
\hyphenpenalty=0


\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
% \usepackage{preamble}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage[framemethod=TikZ]{mdframed}
% \usepackage{../thmboxes_white}
\usepackage{../thmboxes_v2}
\usepackage{float}
% \usepackage{setspace}
\usepackage[nodisplayskipstretch]{setspace}





% \setlength{\parskip}{0pt}

% Custom Definitions of operators
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\send}{send}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Maps}{Maps}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\scale}{sc}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\Tr}{tr}
\DeclareMathOperator{\diag}{diag}

\usepackage{hyperref} % note: this is the final package

\parindent = 0pt

\renewcommand\labelitemi{\tiny$\bullet$}

\begin{document}

\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.5pt}
\setlength{\abovedisplayshortskip}{3.5pt}
\setlength{\belowdisplayshortskip}{3.5pt}

\begin{multicols}{3}
\raggedcolumns

\section{Vector Spaces}
\subsection{Fields and Vector Spaces}

\begin{dfn}[Definition of a field]{dfn:field}{}
    A {field} $F$ is a set with two functions
    \begin{itemize}
        \setlength\itemsep{0em}
        \item Addition: $+ : F \times F \to F,\,(\lambda, \mu) \mapsto \lambda + \mu$
        \item Multiplication: $\cdot : F \times F,\, (\lambda, \mu) \mapsto \lambda\mu$
    \end{itemize}
    which satisfy the following axioms:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $(F, +)$ is an abelian group $F^{+}$, with identity $0_{F}$
        \item $(F\backslash \{0_{F}\}, \cdot)$ is an abelian group $F^{\times}$, with identity $1_{F}$
        \item \textbf{Distributive law}: For all $a$, $b$, and $c$ in $F$, we have
            \[a(b + c) = ab + ac \in F\]
    \end{enumerate}

    \vspace{-5pt}
    and the following lemmas:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item The elements $0_{F}$ and $1_{F}$ of $F$ are distinct
        \item For all $a\in F$, $a \cdot 0_{F} = 0_{F} $ and $0_{F} \cdot a = 0_{F}$
        \item Multiplication in $F$ is associative, and $1_{F}$ is an identity element
    \end{enumerate}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}

    A \textbf{vector space $V$ over a field} $F$ is a pair consisting of an abelian group $V = (V,\, \dot{+})$ and a mapping
    \[F \times V \to V : (\lambda, \vec{v})\mapsto \lambda \vec{v}\]
    s.t. for all $\lambda, \mu \in F$ and $\vec{v}, \vec{w}\in V$ the following identities hold:

    \vspace{-5pt}
    \begin{itemize}
        \setlength\itemsep{0em}
        \item \textbf{Distributivity 1}: $\lambda(\vec{v} \dot{+} \vec{w}) = \lambda\vec{v} \dot{+} \lambda \vec{w}$
        \item \textbf{Distributivity 2}: $(\lambda + \mu)\vec{v} = \lambda \vec{v} \dot{+} \mu \vec{v}$
        \item \textbf{Associativity}: $\lambda (\mu \vec{v}) = (\lambda \mu) \vec{v}$
        \item \textbf{Identity}: $1\vec{v} = \vec{v}$
    \end{itemize}
    \vspace{-5pt}
    and so do the following lemmas:

    \vspace{-5pt}
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item If $V$ is a vector space and $\vec{v}\in V$, then $0 \vec{v} = \vec{0}$
        \item If $V$ is a vector space and $\vec{v}\in V$, then $(-1)\vec{v} = -\vec{v}$
        \item If $V$ is a vector space over a field $F$, then $\lambda \vec{0} = \vec{0}$ for all $\lambda\in F$. Furthermore, if $\lambda \vec{v} = \vec{0}$ then either $\lambda = 0$ or $\vec{v} = \vec{0}$
    \end{enumerate}
\end{dfn}

\subsection{Working with Vector Spaces}

\begin{dfn}[Cartesian Product of $n$ sets]{dfn:cartesian-prod}{}
    \[X_{1} \times \cdots \times X_{n} := \{(x_{1}, \dots, x_{n}) : x_{i}\in X_{i} \text{ for } 1 \le i \le n\}\]

    The elements of a product are called \textbf{$n$-tuples}. An individual entry $x_{i} = (x_{1}, \dots ,x_{n})$ is called a \textbf{component}.

    There are special mappings called \textbf{projections} for a cartesian product:
    \begin{align*}
        \text{pr}_{i} : X_{1} \times \cdots \times X_{n} &\to X_{i}\\
        (x_{1},\dots,x_{n}) &\mapsto x_{i}
    \end{align*}

    The cartesian product of $n$ copies of a set $X$ is written in short as: $X^{n}$
\end{dfn}

\begin{dfn}[Vector Subspace]{dfn:vector-subspace}{}
    A subset $U$ of a vector space $V$ is called a \textbf{vector subspace} or \textbf{subspace} if $U$ contains the zero vector, and whenever $\vec{u},\vec{v}\in U$ and $\lambda\in F$ we have $\vec{u} + \vec{v}\in U$ and $\lambda \vec{u}\in U$
\end{dfn}

\begin{dfn}[Spans and Linear Independence]{dfn:spanning-subspace}{}
    Let $T \subset V$ for some vector space $V$ over a field $F$. Then amongus all subspaces of $V$ that include $T$ there is a smallest subspace
    \[\langle T \rangle = \langle T \rangle_{F} \subseteq V\]
    ``the set of all vectors $\alpha_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r}$ with $\alpha_{1},\dots,\alpha_{r}\in F$ and $\vec{v}_{1},\dots,\vec{v}_{r}\in T$, together with the zero vector in the case $T = \emptyset$''

    \noindent\rule{\textwidth}{0.2pt}

    \textbf{Terminology Dump}
    \begin{itemize}
        \setlength\itemsep{0em}
        \item An expression of the form $\alpha_{1}\vec{v}_{1}+\cdots+\alpha_{r}\vec{v}_{r}$ is called a \textbf{linear combination} of vectors $\vec{v}_{1},\dots,\vec{v}_{r}$
        \item The smallest vector subspace $\langle T \rangle \subseteq V$ containing $T$ is called the \textbf{vector subspace generated by $T$} or the vector subspace \textbf{spanned by $T$} or even the \textbf{span of $T$}
        \item If we allow the zero vector to be the "empty linear combination of $r = 0$ vectors", then the span of $T$ is exactly the set of all linear combinations of vectors from $T$ 
        \item A subset of a vector space that spans the entire space is called a \textbf{generating} or \textbf{spanning set}. A vector space that has a finite generating set is said to be \textbf{finitely generated}
    \end{itemize}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}

    \textbf{Linear Independence}
    \vspace{5pt}

    A subset $L$ of a vector space $V$ is called \textbf{linearly independent} if for all pairwise different vectors $\vec{v}_{1},\dots,\vec{v}_{r}\in L$ and arbitrary scalars $\alpha,\dots,\alpha_{r}\in F$,
    \[a_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r} = \vec{0} \implies a_{1} = \cdots = \alpha_{r} = 0\]
    A subset $L$ of a vector space $V$ is called \textbf{linearly dependent} if it is not linearly independent (duh..). This means there exists pairwise different vectors $\vec{v}j_{1},\dots,\vec{v}_{r}\in L$ and scalars $\alpha_{1},\dots,\alpha_{r}\in F$, not all zero, such that $\alpha_{1}\vec{v}_{1} + \cdots \alpha_{r}\vec{v}_{r} = \vec{0}$
\end{dfn}

\subsection{Linear Independence and Bases}

\begin{dfn}[Basis of a Vector Space]{dfn:basis}{}
    A \textbf{basis of a vector space} $V$ is a linearly independent generating set in $V$
\end{dfn}

\begin{xmp}[Standard Basis]{xmp:standard-basis}{}
    Let $F$ be a field and $n\in \mathbb{N}$. We consider the following vectors in $F^{n}$
    \[\vec{e}_{i} = (0,\dots,0,1,0,\dots,0)\]
    with one $1$ in the $i$-th place and zero everywhere else. Then $\vec{e}_{1} ,\dots, \vec{e}_{n}$ form an ordered basis of $F^{n}$, the so-called \textbf{standard basis of $F^{n}$}
\end{xmp}

\begin{thm}[Linear combinations of basis elements]{thm:linear-combinations-of-basis-elems}{}
    Let $F$ be a field, $V$ a vector space over $F$ and $\vec{v}_{1},\dots,\vec{v}_{r}\in V$ vectors. The family $(\vec{v}_{i})_{1\le i\le r}$ is a basis of $V$ if and only if the following "evaluation" mapping
    \begin{align*}
        \psi : F^{r} &\to V\\
        (\alpha_{1},\dots,a_{r}) &\mapsto a_{1}\vec{v}_{1} + \cdots + \alpha_{r}\vec{v}_{r}
    \end{align*}
    is a bijection

    If we label our ordered family by $\mathcal{A} = (\vec{v}_{1},\dots,\vec{v}_{r})$, then we done the above mapping by
    \[\psi = \psi_{\mathcal{A}} : F^{r}\to V\]
\end{thm}

\vspace{-5pt}
\begin{thm}[Characterisations of Bases]{thm:basis-characterisations}{}
    The following are equivalent for a subset $E$ of a vector space $V$:
    \vspace{-5pt}
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $E$ is a basis, i.e. a linearly independent generating set
        \item $E$ is minimal among all generating sets, meaning that $E \backslash \{\vec{v}\}$ does not generate $V$, for any $\vec{v}\in E$
        \item $E$ is maximal among all linearly independent subsets, meaning that $E \cup \{\vec{v}\}$ is linearly dependent for any $\vec{v}\in V$
    \end{enumerate}

    \vspace{-10pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Corrollary}: Let $V$ be a finitely generated vector space over a field $F$. Then $V$ has a finite basis

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Basis Characterisation Variant}
    \vspace{-5pt}
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item If $L \subset V$ is a linearly independent subset and $E$ is minimal amongst all generating sets of $V$ with the property that $L \subseteq E$, then $E$ is a basis.
        \item If $E \subseteq V$ is a generating set and if $L$ is maximal amongst all linearly independent sets of $V$ with the property $L \subseteq$ $E$, then $L$ is a basis.
    \end{enumerate}
\end{thm}

\vspace{-5pt}
\begin{dfn}[Free Vector Space]{dfn:free-vector-space}{}
    Let $X$ be a set and $F$ a field. The set $\Maps(X, F)$ of all mappings $f: X \to F $ becomes an $F$-vector space with the operations of pointwise addition and multiplication by a scalar. The subset of all mappings which send almost all elements of $X$ to zero is a vector subspace
    \[F\langle X \rangle \subseteq \Maps(X, F)\]
    This subspace is called the \textbf{free vector space on the set $X$}
\end{dfn}

\vspace{-5pt}
\begin{thm}[Variant of Linear Combinations]{thm:lin-combinations-of-basis}{}
    Let $F$ be a field, $V$ be an $F$-vector space and $(\vec{v}_{i})_{i\in I}$ a family of vectors from the vector space $V$. The following are equivalent:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item The family $(\vec{v}_{i})_{i\in I}$ is a basis for $V$
        \item For each $\vec{v}\in V$ there is precisely one family $(a_{i})_{i\in I}$ of elements of $F$, almost all which are zero and such that
            \[\vec{v} = \sum_{i = I} a_{i}\vec{v}_{i}\]
    \end{enumerate}
\end{thm}

\newpage
\subsection{Dimension of a Vector Space}

\begin{thm}[Fundamental Estimate of LinAlg]{thm:fundamental-estimate-linalg}{}
    No linearly independent subset of a given vector has more elements than a generating set. Thus if $V$ is a vector space, $L \subset V$ a linearly independent subset and $E \subseteq V$ a generating set, then
    \[\lvert L \rvert \le \lvert E \rvert\]
\end{thm} 

\begin{thm}[Steinitz Exchange Theorem]{thm:steinitz-exchange}{}
    Let $V$ be a vector space, $L \subset V$ a finite linearly independent subset and $E \subseteq V$ a generating set. Then there is an injection $\phi : L \hookrightarrow E$ such that $(E \backslash \phi(L)) \cup L$ is also a generating set for $V$
    
    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    Let $V$ be a vector space, $M \subseteq V$ a linearly independent subset, and $E \subseteq V$ a generating subset, such that $M \subseteq E$. If $\vec{w}\in V\backslash M$ is a vector $\not\in M$ such that $M \cup \{ \vec{w}\}$ is linearly independent, then there exists $\vec{e}\in E \backslash M$ such that $(E \backslash \{\vec{e}\}) \cup \{\vec{w}\}$ is a generating set
\end{thm}

\begin{thm}[Cardinality of Bases]{thm:cardinality-of-bases}{}
    Let $V$ be a finitely generated vector space.
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $V$ has a finite basis
        \item $V$ cannot have an infinite basis
        \item Any two bases of $V$ have the same number of elements
    \end{enumerate}
\end{thm}

\begin{dfn}[Dimension of a Vector Space]{dfn:dimension-vectorspace}{}
    The cardinality of a basis of a finitely generated vector space $V$ is called the \textbf{dimension} of $V$, written $\dim V$. If $F$ is a field, and we want to denote that we mean dimension as an $F$-vector space, then we write $\dim_{F} V$. If the vector space is not finitely generated, then we say $\dim V = \infty$ and call $V$ \textbf{infinite dimensional}. 
\end{dfn} 

\begin{thm}[Dimension Theorems]{thm:dimension-theorems}{}
    \textbf{Cardinality Criterion for Bases}
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item Each linearly independent subset $L \subset V$ has at most $\dim V$ elements, and if $\lvert L \rvert = \dim V$ then $L$ is a basis
        \item Each generating set $E \subseteq V$ has at least $\dim V$ elements, and if $\lvert E \rvert = \dim V$ then $E$ is a basis
    \end{enumerate}
    \textbf{Dimension Estimate for Vector Subspaces}: A proper vector subspace of a finite dimensional vector space has itself a strictly smaller dimension

    \noindent\rule{\textwidth}{0.2pt}
    If $U \subseteq V$ is a vector subspace of an arbitrary vector space, then we have $\dim U \le \dim V$ and if we have $\dim U = \dim V < \infty$ then it follows that $U = V$
\end{thm}

\subsection{Linear Mappings}

\begin{dfn}[Linear Mappings]{dfn:linear-mappings}{}
    Let $V$, $W$ be vector spaces over a field $F$. A mapping $f : V \to W$ is called \textbf{linear}, or \textbf{$F$-linear}, or even a \textbf{homomorphism of $F$-vector spaces} if for all $\vec{v}_{1},\vec{v}_{2}\in V$ and $\lambda \in F$ we have
    \begin{align*}
        f(\vec{v}_{1} + \vec{v}_{2}) &= f(\vec{v}_{1}) + f(\vec{v}_{2})\\
        f(\lambda \vec{v}_{1}) &= \lambda f(\vec{v}_{1})
    \end{align*}
    A bijective linear mapping is called an \textbf{isomorphism} of vector spaces. If there is an isomorphism between two vector spaces, we call them \textbf{isomorphic}. A homomorphism $V\to V$ is called an \textbf{endomorphism} of $V$. An isomorphism $V\to V$ is called an \textbf{automorphism} of $V$

    \noindent\rule{\textwidth}{0.2pt}
    Two vector subspaces $V_{1}, V_{2}$ of a vector space $V$ are called \textbf{complementary} if addition defines a bijection
    \[V_{1} \times V_{2} \xrightarrow{\sim} V\]
    something about direct sums
\end{dfn}

\vspace{-5pt}
\begin{thm}[Classifying VecSpaces by Dimension]{thm:classifying-vecspaces-by-dim}{}
    Let $n$ be a natural number. Then a vector space over a field $F$ is isomorphic to $F^{n}$ iff it has dimension $n$
\end{thm}

\vspace{-5pt}
\begin{thm}[Linear Mapping and Bases]{thm:linear-maps-and-bases}{}
    Let $V$, $W$ be vector spaces over a field $F$. The set of all homomorphisms from $V$ to $W$ is denoted by
    \[\Hom_{F}(V,W) = \Hom(V,W)\subseteq \Maps(V,W)\]
    Let $B \subset V$ be a basis. Then restriction of a mapping gives a bijection
    \begin{align*}
        \Hom_{F}(V, W) &\xrightarrow{\sim} \Maps(B, W) \\
        f \mapsto f \lvert_{B}
    \end{align*}
\end{thm}

\vspace{-5pt}
\begin{thm}[Inverse Mappings]{thm:inverse-mappings}{}
    \begin{enumerate}[leftmargin=*]
        \setlength\itemsep{0em}
        \item Every injective linear mapping $f : V \hookrightarrow W$ has a \textbf{left inverse}, or a linear mapping $g : W \to V$ s.t. $g \circ f = \id_{V}$
        \item Every surjective linear mapping $f : V \twoheadrightarrow W$ has a \textbf{right inverse}, or a linear mapping $G : W \to V$ s.t. $f \circ g = \id_{W}$
    \end{enumerate}
\end{thm}

% \subsection{Rank Nullity Theorem}

\vspace{-5pt}
\begin{dfn}[Image and Kernel of a map]{dfn:image-kernel}{}
    The \textbf{image} of a linear mapping $f : V \to W$ is the subset $\im(f) = f(V) \subseteq W$. It is a vector subspace of $W$. The preimage of the zero vector of a linear mapping $f : V \to W$ is denoted by:
    \[\ker(f) := f^{-1}(0) = \{v\in V : f(v) = 0\}\]
    and is called the \textbf{kernel} of the linear mapping $f$. The kernel is a subspace of $V$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Mini lemma}: A linear mapping is injective iff its kernel is zero
\end{dfn}

\begin{thm}[Rank-Nullity / Dimension Theorem]{thm:rank-nullity}{}
    Let $f : V \to W$ be a linear mapping between vector spaces. Then:
    \[\dim V = \dim(\ker f) + \dim (\im f)\]
    Dimension of $\im f$= \textbf{rank} of $f$, dimension of $\ker f$ = \textbf{nullity} of $f$

    \noindent\rule{\textwidth}{0.2pt}
    Let $V$ be a vector space, and $U,W\subseteq V$ vector subspaces. Then
    \[\dim(U + W) + \dim(U \cap W) = \dim U + \dim W\]
\end{thm}

\section{Linear Mappings and Matrices}
\subsection{Linear Mappings \texorpdfstring{$F^{m}\to F^{n}$}{Fm to Fn} and Matrices}
\begin{thm}[Linear Maps \texorpdfstring{$F^{m}\to F^{n}$}{Fm to Fn} and Matrices]{thm:linear-maps-matrices}{}
    Let $F$ be a field and let $m,n\in \mathbb{N}$. There is a bijection between the space of linear mappings $F^{m}\to F^{n}$ and the set of matrices with $n$ rows, $m$ columns, and entries in $F$:
    \begin{align*}
        M : \Hom_{F}(F^{m}, F^{n}) &\xrightarrow{\sim} \Mat(n \times m; F)\\
        f &\mapsto [f]
    \end{align*}

    This attaches to each linear mapping $f$ its \textbf{representing matrix} $M(f) := [f]$. The columns of this matrix are the images under $f$ of the standard basis elements of $F^{m}$
    \[[f] := (f(\vec{e}_{1}) \lvert f(\vec{e}_{2}) \rvert \mid \cdots \mid f(\vec{e}_{m}))\]
\end{thm}


% NOTE this could prob just be removed
\begin{dfn}[Matrix Multiplication]{dfn:matrix-multiplication}{}
    Let $n,m,\ell\in\mathbb{N}$, $F$ a field, and let $A\in \Mat(n \times m; F)$ and $B\in \Mat(m \times \ell; F)$ be matrices. The \textbf{product} $A \circ B = AB\in \Mat(n \times \ell;F)$ is the matrix defined by
    \[(AB)_{ik} = \sum_{j = 1}^{m} A_{ij}B_{jk}\]
\end{dfn}

\begin{thm}[Composition of maps to products]{thm:linear-maps-and-mat-prods}{}
    Let $g : F^{\ell}\to F^{m}$ and $f : F^{m} \to F^{n}$ be linear mappings. The representing matrix of their composition is the product of their representing matrices:
    \[[f \circ g] = [f] \circ [g]\]
    
\end{thm}


% NOTE this could prob just be removed
\begin{thm}[Calculating with Matrices]{thm:matrix-calculations}{}
    \vspace{-10pt}
    \begin{multicols}{2}
    \begin{itemize}
        \item (A + A')B = AB + A'B
        \item A(B + B') + AB + AB'
        \item IB = B
        \item AI = A
        \item (AB)C = A(BC)
    \end{itemize}
    \end{multicols}
    \vspace{5pt}
\end{thm}

\newpage

\subsection{Matrix Definitions}

\begin{dfn}[Big def-thm pairs]{dfn:def-thm-pairs}{}
    \textbf{Def}: A matrix $A$ is called \textbf{invertible} if there exists matrices $B$ and $C$ such that $BA = I$ and $AC = I$

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm: Invertible Equivalence}
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item There exists a square matrix $B$ such that $BA = I$
        \item There exists a square matrix $C$ such that $AC = I$
        \item The square matrix $A$ is invertible
    \end{enumerate}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.6pt}
    \textbf{Def}: An \textbf{elementary matrix} is any square matrix that differs from the identity matrix in at least one entry

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: Every square matrix with entries in a field can be written as a product of elementary matrices

    \noindent\rule{\textwidth}{0.6pt}
    \textbf{Def}: Any matrix whose only non-zero entries lie on the diagonal, and which has first $1$'s along the diagonal and then $0$'s, is said to be in \textbf{Smith Normal Form}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: For each matrix $A\in \Mat(n \times m; F)$ there exist invertible matrices $P$ and $Q$ such that $PAQ$ is a matrix in Smith Normal Form
    
    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: Let $f : V \to W$ be a linear map between finite dim. $F$-vector spaces. There exists two ordered bases $\mathcal{A}$ of $V$, and $\mathcal{B}$ of $W$ s.t. the representing matrix ${}_{\mathcal{B}}[f]_{\mathcal{A}}$ has zero entries everywhere except possibly on the diagonal, and along the diagonal there are $1$'s first, followed by $0$'s

    \noindent\rule{\textwidth}{0.6pt}
    \textbf{Def}: The \textbf{column rank} of a matrix $A\in \Mat(n \times m; F)$ is the dimension of the subspace of $F^{n}$ generated by the columns of $A$. Similarly, the \textbf{row rank} of $A$ is the dimension of the subspace of $F^{m}$ generated by the rows of $A$.

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: The column and row rank of any matrix are equal

    \noindent\rule{\textwidth}{0.6pt}
    \textbf{Def}: Since they are both the same, "column" and "row" can be omitted for the \textbf{rank of a matrix}, written as $\rk A$. If the rank is equal to the no. of rows/columns, then the matrix has \textbf{full rank}

    \noindent\rule{\textwidth}{0.6pt}
    \textbf{Def}: The \textbf{trace} of a square matrix is defined to be the sum of its diagonal entries, denoted by $\Tr(A)$
\end{dfn}

\vspace{-5pt}
\subsection{Abstract Linear Mappings and Matrices}

\begin{thm}[Representing Matrices]{thm:representing-matrices}{}
    Let $F$ be a field, $V$ and $W$ vector spaces over $F$ with ordered bases $\mathcal{A} = (\vec{v}_{1},\dots,\vec{v}_{m})$ and $\mathcal{B} = (\vec{w}_{1},\dots,\vec{w}_{n})$. Then to each linear mapping $f : V \to W$ we associate a \textbf{representing matrix} $_{\mathcal{B}}[f]\mathcal{A}$ whose entries $a_{ij}$ are defined by the identity
    \[f(\vec{v}_{j}) = a_{1j}\vec{w}_{1} + \cdots + a_{nj}\vec{w}_{n}\in W\]
    This makes a bijection, which is an isomorphism of vector spaces:
    \begin{align*}
        M^{\mathcal{A}}_{\mathcal{B}} : \Hom_{F}(V, W) &\xrightarrow{\sim} \Mat(n \times m; F) \\
        f &\mapsto {}_{\mathcal{B}}[f]_{\mathcal{A}}
    \end{align*}
\end{thm}

\begin{thm}[Repr. Mat of Compositions]{thm:representing-matrix-of-composition}{}
    Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $kF$ with ordered bases $\mathcal{A}, \mathcal{B}, \mathcal{C}$. If $f : U \to V$ and $g : V \to W$ are linear mappings, then the representing matrix of the composition $g \circ f : U \to W$ is the matrix product of the representing matrices of $f$ and $g$:
    \[{}_{\mathcal{C}}[g \circ f]_{\mathcal{A}} = {}_{\mathcal{C}}[g]_{\mathcal{B}} \circ {}_{\mathcal{B}}[f]_{\mathcal{A}}\]
\end{thm}

\begin{dfn}[Representation of a vector]{dfn:representation-vector}{}
    Let $V$ be a finite dimensional vector space with an ordered basis $\mathcal{A} = (\vec{v}_{1},\dots,\vec{v}_{m})$. We'll denote the inverse to the bijection in \ref{thm:linear-combinations-of-basis-elems} ``$\Phi_{\mathcal{A}} : F^{m} \xrightarrow{\sim} V, (\alpha_{1},\dots,\alpha_{m})^{T} \mapsto \alpha_{1}\vec{v}_{1} +\cdots + \alpha_{m}\vec{v}_{m}$'' by
    \[\vec{v} \mapsto {}_{\mathcal{A}}[\vec{v}]\]
    The column vector ${}_{\mathcal{A}}[\vec{v}]$ is called the \textbf{representation of the vector $\vec{v}$ with respect to the basis $\mathcal{A}$}

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm: Representation of the Image of a Vector}: Let $V,W$ be finite dim. vector spaces over $F$ with ordered bases $\mathcal{A}, \mathcal{B}$ and let $f : V \to W$ be a linear mapping. The following holds for $\vec{v}\in V$:
    \[{}_{\mathcal{B}}[f(\vec{v})] = {}_{\mathcal{B}}[f]_{\mathcal{A}}\circ {}_{\mathcal{A}}[\vec{v}]\]
\end{dfn}

\subsection{Change of a Matrix by Change of Basis}

\begin{dfn}[Change of Basis Matrix]{dfn:change-of-basis}{}
    Let $\mathcal{A} = (\vec{v}_{1},\dots,\vec{v}_{n})$ and $\mathcal{B} = (\vec{w}_{1},\dots,\vec{w}_{n})$ be ordered basies of the same $F$-vector space $V$. Then the matrix representing the identity mapping w.r.t. these bases
    \[{}_{\mathcal{B}}[\id_{V}]_{\mathcal{A}}\]
    is called a \textbf{change of basis matrix}. By definition, its entries are given by the equalities $\vec{v}_{j} = \sum_{i = 1}^{n} a_{ij}\vec{w}_{i}$
\end{dfn}

\begin{thm}[Change of Basis]{thm:change-of-basis}{}
    Let $V$ and $W$ be finite dimensional vector spaces over $F$ and let $f : V \to W$ be a linear mapping. Suppose that $\mathcal{A}, \mathcal{A}'$ are ordered bases of $V$ and $\mathcal{B}, \mathcal{B}'$ are ordered bases of $W$. Then
    \[{}_{\mathcal{B}'}[f]_{\mathcal{A}'} = {}_{\mathcal{B}'}[\id_{W}]_{\mathcal{B}} \circ {}_{\mathcal{B}}[f]_{\mathcal{A}} \circ {}_{\mathcal{A}}[\id_{V}]_{\mathcal{A}'}\]

    \noindent\rule{\textwidth}{0.2pt}
    Let $V$ be a finite dimensional vector space and let $f : V \to V$ be an endomorphim of $V$. Suppose that $\mathcal{A}, \mathcal{A}'$ are ordered bases of $V$. Then
    \[{}_{\mathcal{A}'}[f]_{\mathcal{A}'} = {}_{\mathcal{A}}[\id_{V}]_{\mathcal{A}'}^{-1} \circ {}_{\mathcal{A}}[f]_{\mathcal{A}} \circ {}_{\mathcal{A}}[\id_{V}]_{\mathcal{A}'}\]
\end{thm}

% \vspace{30pt}

\section{Rings and Modules}

\subsection{Ring basics}
\begin{dfn}[Definition of a Ring]{dfn:ring}{}
    A \textbf{ring} is a set with two operations $(\mathbb{R}, +, \cdot)$ that satisfy:

    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $(R, +)$ is an abelian group
        \item $(R, \cdot)$ is a \textbf{monoid}, meaning that it is a set with \textbf{Associativity} and \textbf{Identity}, or in other words, a monoid is a group without the neccessity of having the \textbf{Inverse} axiom
        \item The distributive laws hold, meaning that for all $a,b,c\in R$,
            \begin{align*}
                a \cdot (b + c) &= (a \cdot b) + (a \cdot c)) \\
                (a + b) \cdot c &= (a \cdot c) + (b \cdot c)
            \end{align*}
    \end{enumerate}
    The two operations are called \textbf{addition} and \textbf{multiplication} in our ring. A ring in which multiplication, that is $a \cdot b = b \cdot a$ for all $a,b\in R$, is a \textbf{commutative ring}

    \noindent\rule{\textwidth}{0.2pt}

    \textbf{Note}: We denote the identity of the monoid $(R, \cdot)$ as $1$, and the additive identity of $(R, +)$ as $0_{R}$ or $0$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Note}: We define the \textbf{null ring} or \textbf{zero ring} as a ring where $R$ is a single element set, i.e. $\{0\}$ where $0 + 0 = 0$ and $0 \times 0 = 0$
\end{dfn}


\begin{xmp}[Modulo Rings]{xmp:modulo-rings}{}
    Let $m\in \mathbb{Z}$. Then the set of \textbf{integers modulo} $m$, written
    \[\mathbb{Z} / m\mathbb{Z}\]
    is a ring. The elements of $\mathbb{Z} / m\mathbb{Z}$ consist of \textbf{congruence classes} of integers modulo $m$ - that is, the elements are the subsets $T$ of $\mathbb{Z}$ of the form $T = a + m\mathbb{Z}$ with $a\in \mathbb{Z}$. Think of these as the set of integers that have the same remainder when you divide them by $m$. I denote the above congruence class by $\overline{a}$. Obviously $\overline{a} = \overline{b}$ is the same as $a-b\in m\mathbb{Z}$, and often I'll write
    \[a \equiv b \mod m\]
\end{xmp}

\subsection{Linking Rings to Fields and Further Properties}

\begin{dfn}[Ring definition of a field]{dfn:field-ring}{}
    A \textbf{field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1}\in F$, that is an element $a^{-1}$ with the property that $a \cdot a^{-1} = a^{-1} \cdot a = 1$
\end{dfn}



\begin{dfn}[Multiples of an abelian group]{dfn:abelian-group-multis}{}
    Let $m\in \mathbb{Z}$. The \textbf{$m$-th multiple $ma$ of an element $a$}in an abelian group $R$ is:
    \[ma = \underbrace{a + a + \cdots + a}_{\text{$m$ terms}} \quad \text{if} m > 0\]
    $0a = 0$ and negative multiples are defined by $(-m)a = -(ma)$
\end{dfn}

\newpage


\begin{thm}[Properties of Rings]{dfn:ring-lemmas-1}{}
    \textbf{Lemma set 1}: Let $R$ be a ring and let $a,b\in R$. Then:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $0a = 0 = a 0$
        \item $(-a)b = -(ab) = a(-b)$
        \item $(-a)(-b) = ab)$
    \end{enumerate}

    \textbf{Lemma set 2}: Let $R$ be a ring, $a,b\in R$ and $m,n\in \mathbb{Z}$. Then:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $m(a + b) = ma + mb$
        \item $(m + n)a = ma + na$
        \item $m(na) = (mn)a$
        \item $m(ab) = (ma)b = a(mb)$
        \item $(ma)(nb) = (mn)(ab)$
    \end{enumerate}

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Prime Property for Fields}: Let $m$ be a natural number. The commutative ring $\mathbb{Z} / m\mathbb{Z}$ is a field if and only if $m$ is prime
\end{thm}

\begin{dfn}[Unit of a ring]{dfn:ring-unit}{}
    Let $R$ be a ring. An element $a\in R$ is called a \textbf{unit} if it is \textit{invertible} in $R$ or in other words \textit{has a multiplicative inverse in $R$}, meaning that there exists $a^{-1}\in R$ such that
    \[aa^{-1} = 1 = a^{-1} a\]

    \vspace{-5pt}
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication
\end{dfn}

\begin{dfn}[zero-divisors of a ring]{dfn:zero-divisor}{}
    In a ring $R$, a non-zero element $a$ is called a \textbf{zero-divisor} or \textbf{divisor of zero} if there exists a non-zero element $b$ such that either $ab = 0$ or $ba = 0$.
\end{dfn}

\begin{dfn}[Integral Domain]{dfn:integral-domain}{}
    An \textbf{integral domain} is a non-zero commutative ring that has no zero-divisors. The following two laws hold:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $ab = 0 \implies a = 0 \text{ or } b = 0$
        \item $a \ne 0 \text{ and } b \ne 0 \implies ab \ne 0$
    \end{enumerate}
\end{dfn}


\begin{thm}[Integral Domain Properties]{thm:int-domain-props}{}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item \textbf{Cancellation Law}: Let $R$ be an integral domain and let $a,b,c\in R$. If $ab = ac$ and $a\ne 0$ then $b = c$
        \item Let $m$ be a natural number. Then $\mathbb{Z} / m\mathbb{Z}$ is an integral domain if and only if $m$ is prime.
        \item Every \textbf{finite} integral domain is a \hyperref[dfn:field]{field}.
    \end{itemize}
    
    
\end{thm}

\subsection{Polynomials}

\begin{dfn}[Polynomial]{dfn:polynomial}{}
    Let $R$ be a ring. A \textbf{polynomial over $R$} is an expression of the form
    \[P = a_{0} + a_{1}X + a_{2}X^{2} + \cdots + a_{m}X^{m}\]
    for some non-negative $m\in \mathbb{Z}$ and elements $a_{i}\in R$ for $0 \le i \le m$. 

    \vspace{-5pt}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item The set of all polynomials over $R$ is denoted by $R[X]$.
        \item In the case where $a_{m}$ is non-zero, the polynomial $P$ has \textbf{degree} $m$, (written $\deg(P)$), and $a_{m}$ is its \textbf{leading coefficient}
        \item When the leading coefficient is $1$ the polynomial is a \textbf{monic polynomial}.
        \item A polynomial of degree one is called \textbf{linear}, degree two is called \textbf{quadractic}, and degree three is called \textbf{cubic}.
    \end{itemize}
\end{dfn}

\begin{dfn}[Ring of Polynomials]{dfn:polynomial-rings}{}
    The set $R[X]$ becomes a ring called the \textbf{ring of polynomials with coefficients in $R$, or over $R$}. The zero and the identity of $R[X]$ are the zero and identity of $R$, respectively.
\end{dfn}


\begin{thm}[Properties of a Polynomial Ring]{thm:zero-divisors-of-poly-ring}{}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors and $\deg(PQ) = \deg(P) + \deg(Q)$ for non-zero $P,Q\in R[X]$.
        \item If $R$ is an integral domain, then so is $R[X]$
        \item Let $R$ be an integral domain and let $P, Q\in R[X]$ with $Q$ monic. Then there exists unique $A,B\in R[X]$ such that $P = AQ + B$ and $\deg(B) < \deg(Q)$ or $B = 0$
    \end{itemize}
\end{thm}

\begin{dfn}[Evaluating a Function]{dfn:evaluating-functions}{}
    Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then $P$ can be \textbf{evaluated} at the element $\lambda\in R$ to produce $P(\lambda)$ by replacing the powers of $X$ in $P$ by the corresponding powers of $\lambda$. In this way we have a mapping
    \[R[X] \to \Maps(R, R)\]
    This is the precise definition of thinking of a polynomial as a function. An element $\lambda\in R$ is a \textbf{root} of $P$ if $P(\lambda) = 0$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: Let $R$ be a commutative ring, let $\lambda\in R$ and $P(X)\in R[X]$. Then $\lambda$ is a root of $P(X)$ if and only if $(X - \lambda)$ divides $P(X)$
\end{dfn}

\begin{thm}[Degrees of Polynomial Roots]{thm:polynomial-root-degs}{}
    Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial $P\in R[X] \backslash \{0\}$ has at most $\deg(P)$ roots in $R$
\end{thm}

\begin{dfn}[Algebraically closed fields]{dfn:algebraically=closed}{}
    A field $F$ is \textbf{algebraically closed} if each non-constant polynomial $P\in F[X]\backslash F$ with coefficients in our field has a root in our field $F$
\end{dfn}

\begin{thm}[The Fundamental Theorem of Algebra]{thm:fundamental-theorem-of-algebra}{}
    The field of complex numbers $\mathbb{C}$ is algebraically closed.
\end{thm}

\begin{thm}[Linear Factors of Closed Fields]{thm:alg-closed-fields-linear-factors}{}
    If $F$ is an algebraically closed field, then every non-zero polynomial $P\in F[X]\backslash \{0\}$ \textbf{decomposes into linear factors}
    \[P = c(X - \lambda_{1}) \cdots (X - \lambda_{n})\]
    with $n\ge 0,\, c\in F^{\times}$ and $\lambda_{1},\dots,\lambda_{n}\in F$. This decomposition is unique up to reordering the factors
\end{thm}

\subsection{Homomorphisms, Ideals, and Substrings}

\begin{dfn}[Ring Homomorphisms]{dfn:ring-homomorphisms}{}
    Let $R$ and $S$ be rings. A mapping $f : R \to S$ is a \textbf{ring homomorphism} if the following hold for all $x, y\in R$:
    \begin{align*}
        f (x + y) &= f(x) + f(y)\\
        f(xy) &= f(x)f(y)
    \end{align*}
\end{dfn}

\begin{thm}[Properties of Ring Homomorphisms]{thm:ring-homomorphism-props}{}
    Let $R$ and $S$ be rings and $f : R \to S$ a ring homomorphism. Then for all $x,y\in R$ and $m\in \mathbb{Z}$:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $f(0_{R}) = 0_{S}$, where $0_{R}$ and $0_{S}$ are the zeros of $R$ and $S$
        \item $f(-x) = -f(x)$
        \item $f(x - y)$ = f(x) - f(y)
        \item $f(mx) = mf(x)$
        \item $f(x^{n}) = (f(x))^{n}$ for all $x\in R$ and $n\in \mathbb{N}$
    \end{enumerate}
\end{thm}

\begin{dfn}[Ideal]{dfn:ideal}{}
    A subset $I$ of a ring $R$ is an \textbf{ideal}, $I \unlhd R$, if the following hold:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $I \ne \emptyset$
        \item $I$ is closed under subtraction
        \item for all $i\in I$ and $r\in R$ we have $ri, ir\in I$
    \end{enumerate}
\end{dfn}

\newpage
\begin{dfn}[Generated Ideals]{dfn:ideal-generators}{}
    Let $R$ be a commutative ring and let $T \subset R$. Then the \textbf{ideal of $R$ generated by $T$} is the set
    \[{}_{R}\langle T \rangle = \{r_{1}t_{1}+\cdots+r_{m}t_{m} : t_{1},\dots,t_{m}\in T,\,r_{1},\dots,r_{m}\in R\}\]
\end{dfn}

\begin{thm}[]{thm:generated-ideals-are-smallest-ideal}{}
    Let $R$ be a commutative ring and let $T\subseteq R$. Then ${}_{R}\langle T \rangle$ is the smallest ideal of $R$ that contains $T$
\end{thm}

\begin{dfn}[Principal Ideal]{dfn:principal-ideal}{}
    Let $R$ be a commutative ring. An ideal $I$ of $R$ is called a \textbf{principal ideal} if $I = \langle t \rangle$ for some $t\in R$
\end{dfn}

\begin{thm}[Kernels as Ideals]{thm:kernels-as-ideals}{}

    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item Let $R$ and $S$ be rings and $f : R\to S$ a ring homomorphism. Then $\ker f$ is an ideal of $R$.
        \item $f$ is injective if and only if $\ker f = \{0\}$
        \item The intersection of any collection of ideals of a ring $R$ is an ideal of $R$
        \item Let $I$ and $J$ be ideals of a ring $R$. Then
            \[I + J = \{a + b : a\in I,\, b\in J\}\]
            is an ideal of $R$
    \end{itemize}
\end{thm}

\begin{dfn}[Subrings]{dfn:subrings}{}
    Let $R$ be a ring. $R' \subset R$ is a \textbf{subring} of $R$ if $R'$ is itself a ring under the operations of addition and multiplication defined in $R$.

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm: Test for subring}: Let $R$ be a subset of a ring $R$. Then $R'$ is a subring iff:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $R'$ has a multiplicative identity
        \item $R'$ is closed under subtraction: $a,b\in R' \to a-b\in R'$
        \item $R'$ is closed under multiplication
    \end{enumerate}

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: Let $R$ and $S$ be rings and $f : R \to S$ a ring homomorphism.
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item If $R'$ is a subring of $R$ then $f(R')$ is a subring of $S$. In particular, $\im f$ is a subring of $S$.
        \item Assume that $f(1_{R}) = 1_{S}$. Then if $x$ is a unit in $R$, $f(x)$ is a unit in $S$ and $(f(x))^{-1} = f(x^{-1})$. In this case, $f$ restricts to a group homomorphism $f \rvert_{R^{X}} : R^{X} \to S^{X}$
    \end{enumerate}
\end{dfn}

\subsection{Equivalence Relations}

\begin{dfn}[Equivalence Relations]{dfn:equivalence-relations}{}
    A \textbf{relation} $R$ on a set $X$ is a subset $R \subseteq X \times X $. In the context of relations, it's written $xRy$ instead of $(x,y)\in R$. $R$ is an \textbf{equivalence relation on $X$} when for all elements $x, y, z\in X$ the following hold:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item \textbf{Reflexivity}: $xRx$
        \item \textbf{Symmetry}: $xRy \iff yRx$
        \item \textbf{Transivity}: $xRy \text{ and } yRz \implies xRz$
    \end{enumerate}
\end{dfn}

\begin{dfn}[Equivalence Classes]{dfn:equivalence-classes}{}
    Suppose that $\sim$ is an equivalence relation on a set $X$. For $x\in X$ the set $E(x) := \{z\in X : z \sim x\}$ is called the \textbf{equivalence class of $x$}. A subset $E \subseteq X$ is called an \textbf{equivalence class} for our equivalence relation if there is an $x\in X$ for which $E = E(x)$. An element of an equivalence class is called a \textbf{representive} of the class. A subset $Z \subseteq X$ containing precisely one element from each equivalence class is called a \textbf{system of representatives} for the equivalence relation
\end{dfn}

\begin{dfn}[Set of Equivalence Classes]{dfn:equivalence-class-sets}{}
    Given an equivalence relation $\sim$ on the set $X$ I will denote the \textbf{set of equivalence classes}, which is a subset of the power set $\mathcal{P}(X)$, by
    \[(X / \sim) := \{E(x) : x\in X\}\]
    There is a canonical mapping $\text{can}: X \to (X / \sim),\, x\mapsto E(x)$ (surjection)
\end{dfn}

\subsection{Factor Rings and First Isomorphism Theorem}

\begin{dfn}[Coset]{dfn:coset}{}
    Let $I \unlhd R$ be an ideal in a ring $R$. The set
    \[ x + I := \{x + i : i\in I\} \subseteq R\]
    is a \textbf{coset of $I$ in $R$} or the \textbf{coset of $x$ w.r.t $I$ in $R$}
\end{dfn}

\begin{dfn}[Factor Ring]{dfn:factor-ring}{}
    Let $R$ be a ring, $I \unlhd R$ be an ideal, and $\sim$ the equivalence relation defined by $x \sim y \iff x - y \in I$. Then $R / I$, the \textbf{factor ring of $R$ by $I$} or \textbf{the quotient of $R$ by $I$}, is the set $(R / \sim)$ of cosets of $I$ in $R$
\end{dfn}

\begin{thm}[]{thm:factor-ring-thm}{}
    Let $R$ be a ring and $I \unlhd R$ an ideal. Then $R / I$ is a ring, where the operation of addition is defined by
    \[(x + I) \dot{+} (y + I) = (x + y) + I \qquad \text{for all $x,y\in R$}\]
    and multiplication is defined by
    \[(x + I) \cdot (y + I) = xy + I \qquad \text{for all $x,y\in R$}\]
\end{thm}

\begin{thm}[Universal Property of Factor Rings]{thm:univ-prop-of-factor-rings}{}
    Let $R$ be a ring and $I$ an ideal of $R$
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item The mapping $\text{can}: R \to R / I$ sending $r$ to $r + I$ for all $r\in R$ is a surjective ring homomorphism with kernel $I$
        \item If $f : R \to S$ is a ring homomorphism with $f(I) = \{0_{S}\}$, so that $I \subseteq \ker f$ then there is a unique ring homomorphism $f : R / I \to S$ such that $f = \overline{f} \circ \text{can}$
    \end{enumerate}
\end{thm}

\begin{thm}[First Isomorphism Theorem for Rings]{thm:first-iso-thm-rings}{}
    Let $R$ and $S$ be rings. Then every ring homomorphism $f : R\to S$ induces a ring isomorphism
    \[\overline{f} : R / \ker f \xrightarrow{\sim} \im f\]
\end{thm}

\subsection{Modules}

\begin{dfn}[Module]{dfn:module}{}
    A \textbf{(left) module $M$ over a ring $R$} (or an \textbf{$R$-module}) is a pair consisting of an abelian group $M = (M, \dot{+})$ a mapping
    \begin{align*}
        R \times M &\to M\\
        (r, a) &\mapsto ra
    \end{align*}

    s.t. for all $r, s\in R$ and $a,b\in M$, we have:
    \begin{itemize}
        \setlength\itemsep{0em}
        \item \textbf{Distributivity 1}: $r(a \dot{+} b) = (ra) \dot{+} (rb)$
        \item \textbf{Distributivity 2}: $(r + s)a = (ra) \dot{+} (sa)$
        \item \textbf{Associativity}: $r (sa) = (rs) a$
        \item \textbf{Identity}: $1_{R}a = a$
    \end{itemize}
\end{dfn}


\begin{thm}[Module Lemmas]{thm:module-lemmas}{}
    Let $R$ be a ring and $M$ an $R$-module
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $0_{R}a = 0_{M}$ for all $a\in M$
        \item $r 0_{m} = 0_{M}$ for all $r\in R$
        \item $(-r) a = r(-a) = -(ra)$ for all $r\in R$, $a\in M$
    \end{enumerate}
\end{thm}

\newpage

\begin{dfn}[Module Homomorphisms]{dfn:module-homomorphismsa}{}
    Let $R$ be a ring and let $M$, $N$ be $R$-modules. A mapping $f : M\to N$ is an \textbf{$R$-homomorphism} or \textbf{$homomorphism$} if the following hold for all $a,\in M$ and $r\in R$
    \begin{align*}
        f(a + b) &= f(a) + f(b)\\
        f(ra) &= rf(a)
    \end{align*}

    \noindent\rule{\textwidth}{0.2pt}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item The \textbf{kernel} of $f$ is $\ker f = \{a\in M : f(a) = 0_{N}\}\subseteq M$
        \item The \textbf{image} of $f$ is $\im f = \{f(a) : a\in M\}\subseteq N$
        \item If $f$ is a bijection then it is an \textbf{$R$-module isomorphim} or \textbf{isomorphism}, written $M \cong N$, and say $M$ and $N$ are \textbf{isomorphic}
    \end{itemize}

\end{dfn}

\begin{dfn}[Submodules]{dfn:submodules}{}
    A non-empty subset $M'$ of an $R$-module $M$ is a \textbf{submodule} if $M'$ is an $R$-module with respect to the operations of the $R$-module $M$ \textbf{restricted} to $M'$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm}: Let $R$ be a ring and let $M$ be an $R$-module. A subset $M'$ of $M$ is a submodule if and only if
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $0_{M}\in M'$
        \item $a,b\in M' \implies a - b \in M'$
        \item $r\in R, a\in M' \implies ra\in M'$
    \end{enumerate}
\end{dfn}

\begin{thm}[Submodule lemmas]{thm:submodule-lemmas}{}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item Let $f : M\to N$ be an $R$-homomorphism. Then $\ker f$ is a submodule of $M$ and $\im f$ is a submodule of $N$
        \item Let $R$ be a ring, $M$ an $R$-homomorphism. Then $f$ is injective if and only if $\ker f = \{0_{M}\}$
    \end{itemize}
\end{thm}

\begin{dfn}[Generated Submodules]{dfn:generated-submodules}{}
    Let $R$ be a ring, $M$ an $R$-module nad let $T \subseteq M$. Then the \textbf{submodule of $M$ generated by $T$} is the set
    \[{}_{R}\langle T \rangle = \{r_{1}t_{1} +\cdots+ r_{m}t_{m} : t_{1},\dots,t_{m}\in T, r_{1},\dots,r_{m}\in R\}\]
together with the zero element in the case $T = \emptyset$. If $T = \{t_{1},\dots,t_{n}\}$, a finite set, we write ${}_{R}\langle t_{1},\dots,t_{n} \rangle$ instead of ${}_{R}\langle \{t_{1},\dots,t_{n}\} \rangle$. The module $M$ is \textbf{finitely generated} if it is generated by a finite set: $M = {}_{R}\langle t_{1},\dots,t_{n} \rangle$. It is called \textbf{cyclic} if it is generated by a singleton $M = {}_{R}\langle T\rangle$
\end{dfn}

\begin{dfn}[Generated Submodule lemmas]{dfn:generated-submod-lemmas}{}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item Let $T \subseteq M$. Then ${}_{R}\langle T \rangle$ is the smallest submodule of $M$ that contains $T$
        \item The intersection of any collection of submodules of $M$ is a submodule of $M$.
        \item Let $M_{1}$ and $M_{2}$ be submodules of a $M$. Then
            \[M_{1} + M_{2} = \{a + b : a\in M_{1}, b\in M_{2}\}\]
            is a submodule of $M$
    \end{itemize}
\end{dfn}

\begin{dfn}[Submodule Cosets]{dfn:coset-submodule}{}
    Let $R$ be a ring, $M$ an $R$-module, and $N$ a submodule of $M$. For each $a\in M$ the \textbf{coset of $a$ with respect to $N$ in $M$} is
    \[a + N = \{ a + b : b\in N\}\]
    It is a coset of $N$ in the abelian group $M$ and so is an equivalence class for the equivalence relation $a \sim b \iff a - b \in N$.

    \noindent\rule{\textwidth}{0.2pt}
    Let $M / N$, the \textbf{factor of $N$ by $N$} or the \textbf{quotient of $M$ by $N$} to be the set $(M / \sim)$ of all cosets of $N$ in $M$. This becomes an $R$-module by introducing the operations of addition and multiplication as follows:
    \begin{align*}
        (a + N) \dot{+} (b + N) &= (a + b) + N\\
        r(a + N) &= ra + N
    \end{align*}
    for all $a,b\in M$, $r\in R$.

    \noindent\rule{\textwidth}{0.2pt}
    The zero of $M / N$ is the coset $0_{M /N} = 0_{M} + N$. The negative of $a + N\in M / N$ is the coset $-(a + N) = (-a) + N$

    The $R$-module $M / N$ is the \textbf{factor module} of $M$ by the submodule $N$
\end{dfn}

\begin{thm}[Universal Property of Factor Modules]{thm:univ-prop-factor-mods}{}
    Let $R$ be a ring, let $L$ and $M$ be $R$-modules, and $N$ a submodule of $M$.
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item The mapping $\text{can} : M\to M /N$ sending $a$ to $a + N$ for all $a\in M$ is a surjective $R$-homomorphism with kernel $N$
        \item If $f : M\to L$ is an $R$-homomorphism with $f(N) = \{0_{L}\}$, so that $N \subseteq \ker f$, then there is a unique homomorphism $\overline{f} : M /N \to L$ such that $f = \overline{f} \circ \text{can}$
    \end{enumerate}
\end{thm}

\begin{thm}[First Isomorphism Thm for Modules]{thm:first-iso-thm-modules}{}
    Let $R$ be a ring and let $M$ and $N$ be $R$-modules. Then every $R$-homomorphism $f : M\to N$ induces an $R$-isomorphism
    \[\overline{f} : M / \ker f \xrightarrow{\sim} \im f\]
\end{thm}

\section{Determinants and Eigenvalues Redux}

\subsection{Symmetric Groups}

\begin{dfn}[Symmetric Groups]{dfn:symmetric-groups}{}
    The group of all permutations of the set $\{1,2,\dots,n\}$, also known as bijections from $\{1,2,\dots,n\}$ to itself is denoted by $\mathfrak{S}_{n}$ (but i will just write $S_{n}$ because icba) and called the \textbf{$n$-th symmetric group}. It is a group under composition and has $n!$ elements.

    \noindent\rule{\textwidth}{0.2pt}
    A \textbf{tranposition} is a permutation that swaps two elements of the set and leaves all the others unchanged.
\end{dfn}

\begin{dfn}[Inversions of a permutation]{dfn:inversion}{}
    An \textbf{inversion} of a permutation $\sigma\in S_{n}$ is a pair $(i, j)$ such that $1 \le i < j \le n$ and $\sigma(i) > \sigma(j)$. The number of inversions of the permutation $\sigma$ is called the \textbf{length of $\sigma$} and written $\ell(\sigma)$. In formulas:
    \[\ell(\sigma) = \lvert \{(i,j) : i < j \text{ but } \sigma(i) > \sigma(j)\} \rvert\]
    The \textbf{sign of $\sigma$} is defined to be the parity of the number of inversions of $\sigma$. In formulas:
    \[\sgn(\sigma) = (-1)^{\ell(\sigma)}\]
\end{dfn}

\begin{thm}[Multiplicativity of the sign]{thm:permutation-multiplicativity}{}
    For each $n\in \mathbb{N}$ the sign of a permutation produces a group homomorphism $\sgn : S_{n} \to \{+1, -1\}$ from the symmetric group to the two-element group of signs. In formulas:
    \[\sgn(\sigma\tau) = \sgn(\sigma)\sgn(\tau) \quad \forall \sigma, \tau\in S_{n}\]
\end{thm}

\begin{dfn}[Alternating Group of a Permutation]{dfn:alternating-group}{}
    For $n\in \mathbb{N}$, the set of even permutations in $S_{n}$ forms a subgroup of $S_{n}$ because it is the kernel of the group homomorphism $\sgn : S_{n}\to \{+1, -1\}$. This group is the \textbf{alternating group} and is denoted $A_{n}$
\end{dfn}

\subsection{Determinants}

\begin{dfn}[Determinants - the Leibniz Formula]{dfn:determinants}{}
    Let $R$ be a commutative ring and $n\in \mathbb{N}$. The \textbf{determinant} is a mapping $\det : \Mat(n;R) \to R$ from square matrices with coefficients in $R$ to the ring $R$ that is given by the following formula

    \[A = \begin{pmatrix}
        a_{11} & \cdots & a_{1n}\\
        \vdots & \ddots & \vdots\\
        a_{n1} & \cdots & a_{nn}
    \end{pmatrix} \mapsto \det(A) = \sum_{\sigma\in S_{n}} \sgn(\sigma) a_{1\sigma(1)\cdots}  a_{n\sigma(n)}\]

    The sum is over all permutations of $n$, and the coefficient $\sgn(\lambda)$ is the sign of the permutation $\sigma$ defined above. When $n = 0$, the determinant is $1$
\end{dfn}

\newpage
\subsection{Characterising the Determinant}

\begin{dfn}[Bilinear Forms]{dfn:bilinear-forms}{}
    Let $U,V,W$ be $F$-vector spaces. A \textbf{bilinear form on $U \times V$ with values in $W$} is a mapping $H: U \times V \to W $ which is a linear mapping in both of its entries. This means that it must satisfy the following properties for all $u_{1}, u_{2}\in U$ and $v_{1}, v_{2}\in V$ and all $\lambda\in F$:
    \begin{align*}
        H(u_{1} + u_{2}, v_{2}) &= H(u_{1}, v_{1}) + H(u_{2}, v_{1})\\
        H(\lambda u_{1}, v_{1}) &= \lambda H(u_{1}, v_{1}) \\
        H(u_{1}, v_{2} + u_{2}) &= H(u_{1}, v_{1}) + H(u_{2}, v_{1})\\
        H(u_{1},\lambda v_{1}) &= \lambda H(u_{1}, v_{1}) \\
    \end{align*}
    A bilinear form $H$ is \textbf{symmetric} is $U = V$ and
    \[H(u,v) = H(v,u)\quad \text{for all } u,v\in U\]
    while it is \textbf{antisymmetric} or \textbf{alternating} if $U = V$ and
    \[H(u, u) = 0 \quad\text{for all } u\in U\]

    \noindent\rule{\textwidth}{0.2pt}
    \begin{itemize}
        \item antisymmetric$\implies H(u, v) = -H(v, u)$
        \item $H(u, v) = -H(v, u) \implies \text{antisymmetric}$ iff $1_{F} + 1_{F} \ne 0_{F}$
    \end{itemize}
\end{dfn}

\begin{dfn}[Multilinear Forms]{dfn:multilinear}{}
    Let $V_{1},\dots,V_{n}, W$ be $F$-vector spaces. A mapping $H : V_{1} \times V_{2} \times \cdots \times V_{n} \to W$ is a \textbf{multilinear form} or just \textbf{multilinear} if for each $j$, the mapping $V_{j}\to W$ defined by $v_{j}\mapsto H(v_{1},\dots,v_{j},\dots,v_{n})$, with the $v_{i}\in V_{i}$ arbitrary fixed vectors of $V_{i}$ for $i\ne j$ is linear. 

    \noindent\rule{\textwidth}{0.2pt}
    Let $V$ and $W$ be $F$-vector spaces. A multilinear form $ H : V \times \cdots \times V \to W$ is \textbf{alternating} if it vanishes on every $n$-tuple of elements of $V$ that has at least two entries equal, in other words if:
    \[(\exists i\ne j \text{ with } v_{i} = v_{j})\to H(v_{1},\dots,v_{i},\dots,v_{j},\dots,v_{n}) = 0\]
\end{dfn}

\begin{thm}[Characterisation of the Determinant]{thm:determinant-characterisation}{}
    Let $F$ be a field. The mapping
    \[\det : \Mat(n;F) \to F\]
    is the unique alternating multilinear form on $n$-tuples of column vectors with values in $F$ that takes the value $1_{F}$ on the identity matrix
\end{thm}


% TODO: FIX THE NUMBERING SYSTEM FOR ANY BOX BEFORE THIS
NOTE: numbering past this is accurate to the lecture notes, boxes below this not so much

\subsection{Rules for Calculating with Determinants}

\begin{thm}[Determinant Theorem Bank]{thm:determinant-theorems}{4.4}
    \begin{itemize}
        \item[\textbf{4.4.1}:] Let $R$ be a commutative ring, $A, B\in \Mat(n; R)$. Then
            \[\det(AB) = \det(A)\det(B)\]
        \item[\textbf{4.4.2}:] The determinant of a square matrix with entries in a field $F$ is non-zero if and only if the matrix is invertible
        \item[\textbf{4.4.3}:] \begin{itemize}
            \item If $A$ is invertible then $\det(A^{-1}) = \det(A)^{-1}$
            \item If $B$ is a square matrix then $\det(A^{-1}BA) = \det(B)$
        \end{itemize} 
        \item[\textbf{4.4.4}:] For all $A\in \Mat(n;R)$ with $R$ a commutative ring,
    \[\det(A^{T}) = \det(A)\]

    \end{itemize}
\end{thm}

\begin{dfn}[Cofactors of a Matrix]{dfn:cofactors-matrix}{4.4.6}
    Let $A \in \Mat(n;R)$ for some commutative ring $R$ and $n\in \mathbb{N}$. Let $i,j\in\mathbb{Z}$ between $1$ and $n$. Then the $(i, j)$ \textbf{cofactor of $A$} is $C_{ij} = (-1)^{i + j} \det(A\langle i,j \rangle)$ where $A\langle i, j \rangle$ is the matrix obtained from $A$ by deleting the $i$-th row and $j$-th column.
    \[C_{23} = (-1)^{2 + 3} \det \begin{pmatrix}
        a_{11} & a_{12}& \textcolor{red}{a_{13}}\\
        \textcolor{red}{a_{21}}& \textcolor{red}{a_{22}}& \textcolor{red}{a_{23}}\\
        a_{31}& a_{32}& \textcolor{red}{a_{33}}
    \end{pmatrix} = -a_{11}a_{32} + a_{31}a_{12}\]
\end{dfn}

\begin{thm}[Laplace's Expansion]{thm:laplace-determinant}{4.4.7}
    Let $A = (a_{ij})$ be an $(n \times n)$-matrix with entries from a commutative ring $R$. For a fixed $i$, the \textbf{$i$-th row expansion of the determinant} is
    \[\det(A) = \sum_{j = 1}^{n}a_{ij}C_{ij}\]
    and for a fixed $j$, the \textbf{$j$-th column expansion of the determinant} is
    \[\det(A) = \sum_{i = 1}^{n} a_{ij} C_{ij}\]
\end{thm}

\begin{dfn}[Adjugate Matrix]{dfn:adjugate-matrix}{4.4.8}
    Let $A$ be a $(n \times n)$-matrix with entries in a commutative ring $R$. The \textbf{adjugate matrix} $\text{adj}(A)$ is the $(n \times n)$-matrix whose entries are $adj(A)_{ij} = C_{ji}$ where $C_{ji}$ is the $(j, i)$-cofactor
\end{dfn}

\begin{thm}[Cramer's Rule]{thm:cramers-rule}{4.4.9}
    Let $A$ be a $(n \times n)$-matrix with entries in a commutative ring $R$. Then
    \[A \cdot \text{adj}(A) = (\det A)I_{n}\]
\end{thm}

\begin{thm}[Invertibility of Matrices]{thm:invertibility-of-matrices}{4.4.11}
    A square matrix with entries in a commutative ring $R$ is invertible if and only if its determinant is a unit in $R$. That is, $A\in \Mat(n;R)$ is invertible if and only if $\det(A)\in R^{\times}$
\end{thm}

\begin{thm}[Jacobi's Formula]{thm:jacobis}{4.4.14}
    Let $A = (a_{ij})$ where the coefficients $a_{ij} = a_{ij}(t)$ are functions of $t$. Then
    \[\frac{d}{dt} \det A = \text{Tr}\text{Adj} A \frac{dA}{dt}\]
\end{thm}

\subsection{Eigenvalues and Eigenvectors}

\begin{dfn}[Eigenvalues and Eigenvectors]{dfn:eigenvalue-eigenvector}{}
    Let $f: V \to V $ be an endomorphism of an $F$-vector space $V$. A scalar $\lambda\in F$ is an \textbf{eigenvalue of $f$} if and only if there exists a non-zero vector $\vec{v}\in V$ such that $f(\vec{v}) = \lambda \vec{v}$. Each such vector is called an \textbf{eigenvector of $f$ with eigenvalue $\lambda$}. For any $\lambda\in F$, the \textbf{eigenspace of $f$ with eigenvalue $\lambda$} is
    \[E(\lambda, f) = \{\vec{v}\in V : f(\vec{v}) = \lambda \vec{v}\}\]
\end{dfn}

\begin{thm}[Existence of Eigenvalues]{thm:existence-of-eigenvalues}{4.5.4}
    Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field has an eigenvalue
\end{thm}

\begin{dfn}[Characteristic Polynomial]{dfn:characteristic-polynomial}{4.5.6}
    Let $R$ be a commutative ring and let $A\in \Mat(n;R)$ be a square matrix with entries in $R$. The polynomial $\det(x I_{n} - A)\in R[x]$ is called the \textbf{characteristic polynomial of the matrix $A$}. It is denoted by
    \[\chi_{A}(x) := \det(x I_{n} - A)\]
    (where $\chi$ stands for $\chi$aracteristic, lol)
\end{dfn}

\begin{thm}[EVs and Characteristic Polynomials]{thm:eigenvalues-char-polynomial}{4.5.8}
    Let $F$ be a field and $A\in \Mat(n;F)$ a square matrix with entries in $F$. The eigenvalues of the linear mapping $A : F^{n}\to F^{n}$ are exactly the roots of the characteristic polynomial $\chi_{A}$
\end{thm}

\newpage
\begin{thm}[Eigenvalue Remarks]{thm:eigenvalue-remarks}{4.5.9}
    \begin{itemize}[leftmargin=*]
        \setlength\itemsep{0em}
        \item Square matrices $A, B\in \Mat(n;R)$ of same size are \textbf{conjugate} if
            \[B = P^{-1}AP\in \Mat(n; R)\]
            for an invertible $P\in GL(n;R)$
        \item Conjugacy is an equivalence relation on $\Mat(n;R)$
        \item The char. polynomials for two conjugate matrices are the same
        \item We can define the char. polynomials of an endomorphism $f : V\to V$ of an $n$-dim vector space over a field $F$ to be
            \[\chi_{f}(x) = \chi_{\mathcal{A}}(x)\in F[x]\]
            with $A = {}_{\mathcal{A}}[f]_{\mathcal{A}}\in \Mat(n;R)$ the matrix of $f$ w.r.t \textit{any} basis $\mathcal{A}$ for $V$. The E.V.s of $f$ are exactly the roots of $\chi_{f}$
    \end{itemize}
\end{thm}

\begin{thm}[Extending Bases]{thm:extending-bases}{4.5.10}
    Let $f : V\to V$ be an endomorphism of an $n$-dimensional vector space $V$ over a field $F$. Suppose given an $m$-dimensional subspace $W\subseteq V$ such that $f(W)\subseteq W$, so that there are defined endomorphisms of the subspace and the quotient space:
    \begin{align*}
        g &: W \to W;\, \vec{w}\mapsto f(\vec{w})\\
        h &: V / W \to V / W;\, W + \vec{v} \mapsto W + f(\vec{v})
    \end{align*}
    The characteristic polynomial of $f$ is the product of the characteristic polynomials of $g$ and $h$
    % Any ordered basis $\mathcal{A} = (\vec{w_{1}},\vec{w_{2}},\dots,\vec{w_{m}})$ for $W$ can be extended to an ordered basis for $V$
    % \[\mathcal{B} = (\vec{w_{1}}, \vec{w_{2}},\dots,\vec{w_{m}},\vec{v}_{m+1},\vec{v}_{m+2}\cdots,\vec{v_{n}})\]
    %
    % The images of the $\vec{v}_{j}$'s under the canonical projection $\text{can} : V\to V / W$ are then an ordered basis for $V / W$
    % \[\mathcal{C} = (\text{can}(v_{m + 1}), \text{can}(v_{m + 2}),\dots,\text{can}(\vec{v}_{n}))\]
\end{thm}

\subsection{Triangularisable, Diagonalisable, and Cayley-Hamilton}

\begin{dfn}[Triangularisability]{dfn:triangularisability}{}
    Let $f : V \to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. $f$ is \textbf{triangularisable} if the vector space $V$ has an ordered basis $\mathcal{B} = (\vec{v}_{1}, \vec{v}_{2},\dots,\vec{v}_{n})$ such that
        \begin{align*}
            f(\vec{v}_{1}) &= a_{11}\vec{v_{1}}, \\
            f(\vec{v_{2}}) &= a_{12}\vec{v}_{1} + a_{22}\vec{v}_{2}, \\
            &\vdots \\
            f(\vec{v}_{n}) &= a_{1n}\vec{v}_{1} + a_{2n}\vec{v}_{2} + \cdots + a_{nn}\vec{v}_{n}\in V
        \end{align*}
        (so that the first basis vector $\vec{v}_{1}$ is an eigenvector, with eigenvalue $a_{11}$) or equivalently such that the $n \times n$ matrix $_{\mathcal{B}}[f]_{\mathcal{B}} = (a_{ij})$ representing $f$ with respect to $\mathcal{B}$ is upper triangular (or any other triangular)
        \[A = \begin{pmatrix}
            a_{11}& a_{12}& a_{13}& \cdots & a_{1n} \\
            0& a_{22}& a_{23}& \cdots& a_{2n} \\
            0 & 0& a_{33}& \cdots& a_{3n} \\
            \vdots& \vdots& \vdots& \ddots& \vdots \\
            0 & 0& 0& \cdots& a_{nn}
        \end{pmatrix}\]
\end{dfn}


\begin{thm}[]{thm:triangularisability-chi-poly}{4.6.1 - 4.6.3}
    Let $f : V \to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. Then $f$ is triangularisable iff the characteristic polynomial $\chi_{f}$ decomposes into linear factors in $F[x]$

    \noindent\rule{\textwidth}{0.2pt}
    Finding ordered bases - Choose from the following subspaces
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $W = \{\mu \vec{v}_{1} \mid \mu\in F\}\subseteq V$
        \item $W' = \ker(f - \lambda 1_{V})$. This has a basis of E.Vs $\{\vec{v}_{1},\dots,\vec{v}_{r}\}$
        \item $W'' = \im (\lambda 1_{V} - f)$
    \end{enumerate}
    Then extend the basis to another ordered basis $\mathcal{B}$ for $V$(the full space) where $\text{can}(\vec{v}_{j}) = \vec{u}_{j}$ forms a basis for $V / W$. ${}_{\mathcal{B}}[f]_{\mathcal{B}}$ is upper triangular.

    
    \noindent\rule{\textwidth}{0.2pt}

    An endomorphism $A : F^{n} \to F^{n}$ is triangularisable iff $A = (a_{ij})$ is conjugate to $B = (b_{ij})(b_{ij} = 0 \text{ for $i > j$})$, an upper triangular matrix, with $P^{-1} AP=B$ for an invertible matrix $P$
\end{thm}

\begin{dfn}[Diagonalisability]{dfn:diagonal}{4.6.6}
    An endomorphism $f : V \to V$ of an $F$-vector space $V$ is \textbf{diagonalisable} iff there exists a basis of $V$ consisting of eigenvectors of $f$. If $V$ is finite dimensional then this is the same as saying that there exists an ordered basis $\mathcal{B} = \{\vec{v}_{1},\dots,\vec{v}_{n}\}$ where $_{\mathcal{B}}[f]_{\mathcal{B}} = \text{diag}(\lambda_{1},\dots,\lambda_{n})$. In this case, of course, $f(\vec{v}_{i}) = \lambda_{i}\vec{v}_{i}$.
    
    \noindent\rule{\textwidth}{0.2pt}
    A square matrix $A\in \Mat(n;F)$ is \textbf{diagonalisable} iff $A$ is conjugate to a diagonal matrix, i.e. there exists $P \in \text{GL}(n;F)$ such that $P^{-1}AP = \text{diag}(\lambda_{1},\dots,\lambda_{n})$. In this case the columns $P$ are the vectors of a basis of $F^{n}$ consisting of eigenvectors of $A$ with eigenvalues $\lambda_{1},\dots,\lambda_{n}$
\end{dfn}

\begin{thm}[Linear Independence of Eigenvectors]{thm:ev-linear-independence}{4.6.9}
    Let $f : V\to V$ be an endomorphism of a vector space $V$ and let $\vec{v}_{1},\dots,\vec{v}_{n}$ be eigenvectors of $f$ with pairwise different eigenvalues $\lambda_{1},\dots,\lambda_{n}$. Then the vectors $\vec{v}_{1},\dots,\vec{v}_{n}$ are linearly independent
\end{thm}

\begin{thm}[Cayley-Hamilton Theorem]{thm:cayley-hamilton}{4.6.10}
    Let $A\in \Mat(n;R)$ be a square matrix with entries in a commutative ring $R$. Then evaluating its characteristic polynomial $\chi_{A}(x)\in R[x]$ at the matrix $A$ gives zero.
\end{thm}

\subsection{Markov Matrices}

\begin{dfn}[Markov Matrix]{dfn:markov-matrix}{4.7.5}
    A matrix $M$ whose entires are non-negative and s.t. the sum of the entries of each column equals $1$ is a \textbf{Markov matrix} or a \textbf{stochastic matrix}

    \noindent\rule{\textwidth}{0.2pt}
    \begin{itemize}
        \setlength\itemsep{0em}
        \item[\textbf{4.7.6:}] Suppose $M\in \Mat(n;\mathbb{R})$ is a M.M. Then $\lambda = 1$ is an e.v.
    \end{itemize}
\end{dfn}

\begin{thm}[Perron-Frobenius Theorem]{thm:perron-frobenius}{4.7.10}
    If $M\in \Mat(n;\mathbb{R})$ is a Markov matrix with positive values, then the eigenspace $E(1, M)$ is one-dimensional. There exists a unique basis vector $\vec{v}\in E(1,M)$ with positive real entries s.t. the sum of its entries is $1$
\end{thm}

\section{Inner Product Spaces}

\subsection{Inner Product Spaces Intro}

\begin{dfn}[Inner Product]{dfn:inner-product}{5.1.1}
    Let $V$ be a vector space over $\mathbb{R}$. An \textbf{inner product} on $V$ is a mapping
    \[(- , - ) : V \times V \to \mathbb{R}\]
    that satisfies the following for all $\vec{x}, \vec{y}, \vec{z}\in V$ and $\lambda,\mu\in \mathbb{R}$:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $\lambda\vec{x}+\mu\vec{y},z = \lambda(\vec{x},\vec{z} + \mu(\vec{y},\vec{z}))$
        \item $(\vec{x},\vec{y}) = (\vec{y}, \vec{x})$
        \item $(\vec{x},\vec{x}) \ge 0$, with equality iff $\vec{x} = \vec{0}$
    \end{enumerate}
    A \textbf{real inner product space} is a real vector space equipped with an inner product. \textbf{Note}: basically a generalisation of dot prod.

    \noindent\rule{\textwidth}{0.2pt}
    A \textbf{complex inner product space} is a complex vector space equipped with an inner product. This is the exact same, but condition $2$ uses $(\vec{x},\vec{y}) = \overline{(\vec{y}, \vec{x})}$ where $\overline{z}$ is the complex conjugate
\end{dfn}

\begin{dfn}[Norm]{dfn:norm}{5.1.5}
    In a real or complex inner product space, the \textbf{length} or \textbf{inner product norm} or \textbf{norm} $\lVert \vec{v} \rVert\in \mathbb{R}$ of a vector $\vec{v}$ is defined as the non-negative square root
    \[\lVert \vec{v} \rVert = \sqrt{(\vec{v}, \vec{v})}\]
    Vectors whose length are $1$ are called \textbf{units}. Two vectors $\vec{v}, \vec{w}$ are \textbf{orthogonal}, written $\vec{v} \bot \vec{w}$, iff $(\vec{v}, \vec{w}) = 0$

    \noindent\rule{\textwidth}{0.2pt}
    The norm $\lVert \cdot \rVert$ on an inner product spacse $V$ satisfies, for any $\vec{v}, \vec{w}\in V$ and scalar $\lambda$:
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $\lVert \vec{v} \rVert \ge 0$ with equality iff $\vec{v} = \vec{0}$
        \item $\lVert \lambda \vec{v} \rVert = \lvert \lambda \rvert \lVert  \vec{v} \rVert$
        \item $\lvert \vec{v} + \vec{w} \rvert \le \lVert \vec{v} \rVert + \lVert \vec{w} \rVert$ (triangle inequality)
    \end{enumerate}
\end{dfn}

\newpage
\begin{dfn}[Orthonormal Family]{dfn:orthonormal-family}{5.1.7}
    A family $(\vec{v}_{i})_{i\in I}$ for vectors from an inner product space is an \textbf{orthonormal family} if all the vectors $\vec{v}_{i}$ have length $1$ and if they are pairwise orthogonal to each other, which, if $\delta_{i,j}$ is the \textbf{Kronecker delta} defined by
    \[\delta_{i, j} = \begin{cases}
        1 & i = j \\
        0 & \text{otherwise}
    \end{cases}\]
    means that $(\vec{v}_{i}, \vec{v}_{j}) = \delta_{ij}$.

    An orthonormal family that has a basis is an \textbf{orthonormal basis}
    
    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Thm 5.1.10}: Every finite dimensional inner product space has an orthonormal basis
\end{dfn}

\subsection{Orthogonal Complements and Projections}

\begin{dfn}[Orthogonals to a Subset]{dfn:orthogonals}{5.2.1}
    Let $V$ be an inner product space and let $T \subseteq V$ be an arbitrary subset. Define
    \[T^{\bot} = \{\vec{v} \in V : \vec{v} \bot \vec{t} \, \forall \vec{t}\in T\}\]
    calling this set the \textbf{orthogonal} to $T$
\end{dfn}

\begin{thm}[Complementary Othorgonals]{thm:complementary-orthogonals}{5.2.2}
    Let $V$ be an inner product space and let $U$ be a finite dimensional subspace of $V$. Then $U$ and $U^{\bot}$ are complementary in the sense of \ref{dfn:linear-mappings}. i.e. $V = U \oplus U^{\bot}$
\end{thm}

\begin{dfn}[Orthogonal Projection]{dfn:orthogonal-projection}{5.2.3}
    Let $U$ be a finite dimensional subspace of an inner product space $V$. The space $U^{\bot}$ is the \textbf{orthogonal complement to $U$}. The \textbf{orthogonal projection from $V$ onto $U$} is the map
    \[\pi_{U} : V \to V\]
    that sends $\vec{v} = \vec{p} + \vec{r}$ to $\vec{p}$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Prop 5.2.4}: Let $U$ be a finite dimensional subspace of an inner product space $V$ and let $\pi_{U}$ be the orthogonal projection from $V$ onto $U$
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item $\pi_{U}$ is a linear mapping with $\im(\pi_{U}) = U$ and $\ker (\pi_{U}) = U^{\bot}$
        \item If $\{\vec{v}_{1},\dots,\vec{v}_{n}\}$ is an orthonormal basis of $U$, then $\pi_{U}$ is given by the following formula for all $\vec{v}\in V$
            \[\pi_{U}(\vec{v}) = \sum_{i = 1}^{n} (\vec{v}, \vec{v}_{i}) \vec{v}_{i}\]
        \item $\pi_{U}^{2} = \pi_{U}$, that is, $\pi_{U}$ is an idempotent
    \end{enumerate}
\end{dfn}

\begin{thm}[Cauchy-Shwarz Inequality]{thm:cauchy-shwarz}{5.2.5}
    Let $\vec{v}$, $\vec{w}$ be vectors in an inner product space. Then
    \[\lvert (\vec{v}, \vec{w}) \rvert \le \lVert \vec{v} \rVert \lVert \vec{w} \rVert\]
    with equality if and only if $\vec{v}$ and $\vec{w}$ are linearly dependent
\end{thm}

\begin{thm}[Gram-Shmidt Process]{thm:gram-shmidt}{5.2.7}
    Let $\vec{v}_{1},\dots,\vec{v}_{k}$ be linearly independent vectors in an inner product space $V$. Then there exists an orthonormal family $\vec{w}_{1},\dots,\vec{w}_{k}$ with the property that for all $1 \le i \le k$,
    \[\vec{w}_{i} \in \mathbb{R}_{>0} \vec{v}_{i} + \langle \vec{v}_{i - 1},\dots,\vec{v}_{1} \rangle\]

    \noindent\rule{\textwidth}{0.2pt}
    TODO: write how to actually do the gram-shmidt process
\end{thm}

\subsection{Adjoints and Self-Adjoints}

\begin{dfn}[Adjoints]{dfn:adjoints}{5.3.1}
    Let $V$ be an inner product space. Then two endomorphisms $T, S : V\to V$ are called \textbf{adjoint} to one another if the following holds for all $\vec{v}, \vec{w}\in V$:
    \[(T \vec{v}, \vec{w}) = (\vec{v}, S\vec{w})\]
    In this case I will write $S = T^{*}$ and call $S$ the \textbf{adjoint} of $T$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Remark 5.3.2}: Any endomorphism has at most one adjoint.
\end{dfn}

\begin{thm}[]{thm:adjoints-uniqueness}{5.3.4}
    Let $V$ be a finite dimensional inner product space. Let $T : V \to V$ be an endomorphism. Then $T^{*}$ exists. That is, there is a unique linear mapping $T^{*} : V \to V$ such that for all $\vec{v}, \vec{w}\in V$:
    \[(T \vec{v}, \vec{w}) = (\vec{v}, T^{*}\vec{w})\]
\end{thm}

\begin{dfn}[Self Adjoints]{thm:self-adjoints}{5.3.5}
    An endomorphism of an inner product space $T : V \to V$ is \textbf{self-adjoint} if it equals its own adjoint, i.e. if $T^{*} = T$
\end{dfn}

\begin{thm}[Self-Adjoint Theorem bank]{thm:self-adjoint-thms}{5.3.7}
    Let $T : V\to V$ be a self-adjoint linear mapping on an inner product space $V$
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item Every eigenvalue of $T$ is real
        \item If $\lambda$ and $\mu$ are distinct eigenvalues of $T$ with corresponding eigenvectors $\vec{v}$ and $\vec{w}$, then $(\vec{v}, \vec{w}) = 0$
        \item $T$ has an eigenvalue
    \end{enumerate}
\end{thm}

\begin{dfn}[Orthogonal Matrices]{dfn:orthogonal-matrices}{5.3.11}
    An \textbf{Orthogonal matrix} is an $(n \times n)$-matrix $P$ with real entries such that $P^{T}P = I_{n}$, or in other words such that $P^{-1} = P^{T}$
\end{dfn}

\begin{dfn}[Complex Matrices]{dfn:unitary-matrices}{4.3.14}
    A \textbf{hermitian matrix} is one that is self-adjoint in $\mathbb{C}$, or in other words one where $A = \overline{A}^{T}$ holds


    An \textbf{unitary matrix} is an $(n \times n)$-matrix $P$ with complex entries such that $\overline{P}^{T}P = I_{n}$, or such that $P^{-1} = \overline{P}^{T}$
\end{dfn}

\begin{thm}[Spectral Theorems]{spectral-thm-self-adjoint}{5.3.9}
    \textbf{5.3.9}: \underline{\smash{The Spectral Theorem for Self-Adjoint Endomorphisms}}
    
    Let $V$ be a finite dimensional inner product space and let $T : V \to V$ be a self-adjoint linear mapping. Then $V$ has an orthonormal basis consisting of eigenvalues of $T$.

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{5.3.11}: \underline{\smash{The Spectral Theorem for Real Symmetric Matrices}}

    Let $A$ be a real $(n \times n)$-symmetric matrix. Then there is an $(n \times n)$-orthogonal matrix $P$ such that
    \[P^{T} A P = P^{-1}AP = \diag(\lambda_{1},\dots,\lambda_{n})\]
    where $\lambda_{1},\dots,\lambda_{n}$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of $\chi_{A}$

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{5.3.15}: \underline{\smash{The Spectral Theorem for Hermitian Matrices}}

    Let $A$ be a $(n \times n)$-hermitian matrix. Then there is an $(n \times n)$-unitary matrix $P$ such that
    \[\overline{P}^{T} A P = P^{-1}AP = \diag(\lambda_{1},\dots,\lambda_{n})\]
    where $\lambda_{1},\dots,\lambda_{n}$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of $\chi_{A}$
\end{thm}

\section{Jordan Normal Form}

\subsection{Motivation}

no time for motivation over here

\subsection{The Jordan Normal Form}

% \begin{dfn}[Exponential Map]{dfn:exponential}{6.1.0}
%     The exponential map is defined as
%     \begin{align*}
%         \text{exp} : \Mat(n ; C) &\to \Mat(n ; \mathbb{C})\\
%         A &\mapsto \sum_{k = 0}^{\infty} \frac{1}{k!} A^{k}
%     \end{align*}
% \end{dfn}

\begin{dfn}[Jordan Blocks]{dfn:jordan-blocks}{6.2.1}
    Given an integer $r \ge 1$ define an $(r \times r)$-matrix $J(r)$ called the \textbf{nilpotent Jordan block of size $r$}, by the rule $J(r)_{ij} = 1$ for $j = i + 1$ AND $J(r)_{ij} = 0$ otherwise

    In particular, $J(1)$ is a $(1 \times 1)$-matrix whose only entry is zero.

    \noindent\rule{\textwidth}{0.2pt}
    Given an integer $r \ge 1$ and a scalar $\lambda\in F$, define an $(r \times r)$-matrix $J(r, \lambda)$ called the \textbf{Jordan block of size $r$ and eigenvalue $\lambda$} by the rule
    \[J(r, \lambda) = \lambda I_{r} + J(r) = D + N\]
    with $\lambda I_{r} = \text{diag}(\lambda, \lambda,\dots, \lambda) = D$ diagonal and $J(r) = N$ nilpotent such that $DN = ND$
\end{dfn}

\newpage

\begin{thm}[Jordan Normal Form]{thm:jordan-normal-form}{6.2.2}
    Let $F$ be an algebraically closed field. Let $V$ be a finite dimensional vector space and let $\phi : V \to V$ be an endomorphism of $V$ with characteristic polynomial
    \[\chi_{\phi}(x) = (x - \lambda_{1})^{a_{1}}(x - \lambda_{2})^{a_{2}} . . (x - \lambda_{s})^{a_{s}}\in F[x], a_{i} \ge 1, \sum_{i = 1}^{s} a_{i} = n\]
    For distinct $\lambda_{1},\lambda_{2},\dots,\lambda_{s}\in F$. Then there exists an ordered basis $\mathcal{B}$ of $V$ such that the matrix of $\phi$ with respect to the block $\mathcal{B}$ is block diagonal with Jordan blocks on the diagonal, ${}_{\mathcal{B}}[\phi]_{\mathcal{B}}$
    \[ = \diag(J(r_{11}, \lambda_{1}),\dots,J(r_{1m_{1}}, \lambda_{1}), J(r_{21}, \lambda_{2}),\dots,J(r_{sm_{s}}, \lambda_{s}))\]
    with $r_{11},\dots,r_{1m_{1}}, r_{21,\dots,r_{sm_{s}}} \ge 1$ such that
    \[a_{i} = r_{i_{1}} + r_{i_{2}} + \cdots + r_{im_{i}} \quad (1 \le i \le s)\]
\end{thm}

\begin{thm}[Bézout’s identity for polynomials]{thm:jnf-bezout}{6.3.1}
    For a characteristic polynomial
    \[\chi_{\phi}(x) = \prod_{i = 1}^{s} (x - \lambda_{i})^{a_{i}}\in F[x]\]
    where each $a_{i}$ is a positive integer, $\lambda_{i} \ne \lambda_{j}$ for $i\ne j$, and $\lambda_{i}$ are e.v.s of $\phi$. For each $1\le j \le s$ define
    \[P_{j}(x) = \prod_{\substack{i = 1 \\ i \ne j}}^{s} (x - \lambda_{i})^{a_{i}}\]
    There exists polynomials $Q_{j}(x)\in F[x]$ such that
    \[\sum_{j = 1}^{s} P_{j}(x)Q_{j}(x) = 1\]
\end{thm}

\begin{dfn}[Generalised Eigenspace]{dfn:generalised-eigenspace}{6.3.2}
    The \textbf{generalised eigenspace} of $\phi$ with eigenvalue $\lambda_{i}$, $E^{\text{gen}}(\lambda_{i}, \phi)$ is the following subspace of $V$:
    \[E^{\text{gen}}(\lambda_{i}, \phi) = \{\vec{v}\in V \mid (\phi - \lambda_{i} \id_{V})^{a_{i}} (\vec{v}) = \vec{0}\}\]
    The dimension of $E^{\text{gen}}(\lambda_{i}, \phi)$ is called the \textbf{algebraic multiplicity of $\phi$ with eigenvalue $\lambda_{i}$} while the dimension of the eigenspace $E(\lambda_{i}, \phi)$ is called the \textbf{geometric multiplicity of $\phi$ with eigenvalue $\lambda$}

    \noindent\rule{\textwidth}{0.2pt}
    \textbf{Remark 6.3.4}: The actual eigenspace is defined by
    \[E(\lambda_{i}, \phi) = \{\vec{v}\in V \mid (\phi - \lambda_{i} \id_{V}) (\vec{v}) = \vec{0}\}\]
    $E^{\text{gen}}(\lambda_{i}, \phi) \subseteq E^{\text{gen}}(\lambda_{i}, \phi)$, or the algebraic multiplicity of any e.v. must be greater or equal to the corresponding geometric multiplicity


\end{dfn}

\begin{dfn}[Stable subsets]{dfn:stable-subset}{6.3.4}
    Let $f : X \to X$ be a mapping from a set $X$ to itself. A subset $Y \subseteq X$ is \textbf{stable under $f$} precisely when $f(Y) \subseteq Y$, that is if $y\in Y$ then $f(y)\in Y$.
\end{dfn}

\begin{thm}[Direct Sum Composition]{thm:direct-sum-composition}{6.3.5}
    For each $1 \le i \le s$, let 
    \[\mathcal{B}_{i} = \{\vec{v}_{ij}\in V \mid 1 \le j \le a_{i}\}\]
    be a basis of $E^{\text{gen}}(\lambda_{i}, \phi)$, where $a_i$ is the algebraic multiplicity of $\phi$ with eigenvalue $\lambda_{i}$ s.t. $\sum_{i = 1}^{s} a_{i} = n$ is the dimension of $V$.
    \begin{enumerate}
        \setlength\itemsep{0em}
        \item Each $E^{\text{gen}}(\lambda_{i}, \phi)$ is stable under $\phi$
        \item For each $\vec{v}\in V$ there exist unique $\vec{v}_{i}\in E^{\text{gen}}(\lambda_{i}, \phi)$ such that $\vec{v} = \sum_{i = 1}^{s} \vec{v}_{i}$. In other words, there is a direct sum decomposition
            \[V = \bigoplus_{i = 1}^{s} E^{\text{gen}}(\lambda_{i}, \phi)\]
            with $\phi$ restricting to endomorphisms of the summands
            \[\phi_{i} = \phi \rvert : E^{\text{gen}}(\lambda_{i}, \phi) \to E^{\text{gen}}(\lambda_{i}, \phi)\]
        \item Then
            \[\mathcal{B} = \mathcal{B}_{1} \cup\mathcal{B}_{2} \cup \cdots \cup \mathcal{B}_{s} = \{\vec{v}_{ij} \mid 1 \le i \le s, 1 \le j \le a_{i}\} \]
            is a basis of $V$. The matrix of the endomorphism $\phi$ w.r.t. this basis is given by the block diagonal matrix
            \[{}_{\mathcal{B}}[\phi]_{\mathcal{B}} = \left(\begin{array}{c|c|c|c}
                B_{1} & 0 & 0 & 0 \\
                \hline
                0 & B_{2} & 0 & 0 \\
                \hline
                0 & 0 & \ddots & 0 \\
                \hline
                0 & 0 & 0 & B_{s}
            \end{array}\right) \in \Mat(n ; F)\]
            with $B_{i} = {}_{\mathcal{B}_{i}}[\phi_{i}]_{\mathcal{B}_{i}} \in \Mat(a_{i}; F)$
    \end{enumerate}
\end{thm}

\begin{thm}[JNF Theorem Bank]{thm:jnf-theorems}{6.3}
    \begin{itemize}
        \setlength\itemsep{0em}
        \item[\textbf{6.3.6}:] For each $i$, define a linear mapping
            \[\psi_{i} : \frac{W_{i}}{W_{i - 1}} \to \frac{W_{i - 1}}{W_{i - 2}}\]
            by $\psi_{i}(\vec{w} + W_{i - 1}) = \psi(\vec{w}) + W_{i - 2}$ for $\vec{w}\in W_{i}$. Then $\psi_{i}$ is well-defined and injective
        \item[\textbf{6.3.7}:] Let $f : X \to Y$ be an injective linear mapping between the $F$-vector spaces $X$ and $Y$. If $\{\vec{x}_{1},\dots,\vec{x}_{t}\}$ is a linearly independent set in $X$, then $\{f(\vec{x}_{1},\dots,\vec{x}_{t})\}$ is a linearly independent set in $Y$
        \item[\textbf{6.3.8}:] The set of elements $\{\vec{v}_{j,k} : 1 \le j \le m, 1 \le k \le d_{j}\}$ constructed in the next algorithm is a basis for $W$
        \item[\textbf{6.3.9}:] Let $\mathcal{B}$ be the ordered basis of $W$ - $\{\vec{v}_{j,k} : 1 \le j \le m, 1 \le k \le d_{j}\}$. Then ${}_{\mathcal{B}}[\psi]_{\mathcal{B}} = $
            \[\diag \underbrace{J(m),. .,J(m)}_{\text{$d_{m}$ times}}, \underbrace{J(m-1),. .,J(m-1)}_{\text{$d_{m - 1} - d_{m}$ times}},. .,\underbrace{J(1), . .,J(1)}_{\text{$d_{1} - d_{2}$ times}}\]
            where $J(r)$ denotes the nilpotent Jordan block of size $r$
    \end{itemize}
\end{thm}


% NOTE JNF algorithm

\begin{thm}[JNF Basis Algorithm]{thm:jnf-algorithm}{6.3}
    Algorithm to construct a basis for each $W_{i} / W_{i - 1}$:
    \begin{itemize}[]
        \item Choose an arbitrary basis for $W_{m} / W_{m - 1}$, say
            \[\{v_{m, 1} + W_{m - 1}, \vec{v}_{m, 2} + W_{m - 1},\dots,\vec{v}_{m}, d_{m} + W_{m - 1}\}\]
        \item Since $\psi_{m} : W_{m} / W_{m - 1} \to W_{m - 1} / W_{m - 2}$ is injective by 6.3.6, 6.3.7 proves that 
            \[\{\psi(\vec{v}_{m,1}) + W_{m - 2}, \psi(\vec{v}_{m}, 2) + W_{m - 2}, . . ,\psi(\vec{v}_{m}, d_{m} + W_{m - 2})\}\]
            is a linearly independent set in $W_{m - 1} / W_{m - 2}$. Set $\vec{v}_{m - 1, i} = \psi(\vec{v}_{m, i})$ for $1 \le i \le d_{m}$
        \item Choose vectors $\{\vec{v}_{m - 1, i} : d_{m} + 1 \le i \le d_{m - 1}\}$ so that $\{\vec{v}_{m - 1, i} + W_{m - i - 1} : 1 \le k \le d_{m - i}\}$ is a basis of $W_{m - 1} / W_{m - 2}$
        \item Repeat!
    \end{itemize}
\end{thm}

\subsection{PageRank, again}

\begin{thm}[]{thm:jnf-markov}{6.5.1}
    If $M\in \Mat(n ; \mathbb{R})$ is a Markov matrix with all positive entries, consider $M$ as a complex matrix whose entries just happen to be real. If $\lambda\in \mathbb{C}$ is an eigenvalue of $M$ then either $\lambda = 1$ or $\lvert \lambda \rvert < 1$
\end{thm}

\lipsum[1-2]

\end{multicols}

\end{document}
